{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "# Step 4: Define functions for transformations\n",
    "def safe_parse_labels(labels):\n",
    "    \"\"\"Safely parse labels from a string or return them as-is if already a list.\"\"\"\n",
    "    if isinstance(labels, str):  # If it's a string, evaluate it\n",
    "        try:\n",
    "            return ast.literal_eval(labels)\n",
    "        except (ValueError, SyntaxError):\n",
    "            return []\n",
    "    elif isinstance(labels, list):  # If it's already a list, return as is\n",
    "        return labels\n",
    "    return []\n",
    "\n",
    "def remap_techniques(labels, updates):\n",
    "    \"\"\"Remap techniques to updated equivalents using manual_updates.\"\"\"\n",
    "    return [updates.get(tech, tech) for tech in labels]\n",
    "\n",
    "def convert_and_deduplicate(labels):\n",
    "    \"\"\"Convert sub-techniques to parent techniques and remove duplicates.\"\"\"\n",
    "    return list(set([tech.split('.')[0] if '.' in tech else tech for tech in labels]))\n",
    "\n",
    "def map_techniques_to_tactics(labels, mapping):\n",
    "    \"\"\"Map techniques to their corresponding tactics.\"\"\"\n",
    "    mapped_tactics = []\n",
    "    for tech in labels:\n",
    "        mapped_tactics.extend(mapping.get(tech, []))\n",
    "    return list(set(mapped_tactics))  # Remove duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "import os\n",
    "\n",
    "def map_to_tactics(file_path, predicted_header):\n",
    "    # Load the original dataset\n",
    "    original_df = pd.read_csv(file_path)\n",
    "\n",
    "    # Load the updated MITRE ATT&CK Enterprise data\n",
    "    updated_mitre_data_path = 'enterprise-attack-v16.1 (1).xlsx'\n",
    "    updated_mitre_data = pd.read_excel(updated_mitre_data_path, sheet_name=None)\n",
    "\n",
    "    # Extract necessary sheets\n",
    "    updated_techniques_df = updated_mitre_data['techniques']\n",
    "    tactics_df = updated_mitre_data['tactics']\n",
    "\n",
    "    # Step 1: Create a mapping from techniques to tactics\n",
    "    technique_to_tactics_mapping = {}\n",
    "    for index, row in updated_techniques_df.iterrows():\n",
    "        technique_id = row['ID']\n",
    "        if pd.notnull(row['tactics']):\n",
    "            tactics = [tactic.strip() for tactic in row['tactics'].split(',')]\n",
    "            tactic_ids = [tactics_df[tactics_df['name'] == tactic]['ID'].values[0] for tactic in tactics]\n",
    "            technique_to_tactics_mapping[technique_id] = tactic_ids\n",
    "\n",
    "    # Step 2: Define user-provided manual remappings for deprecated techniques\n",
    "    manual_updates = {\n",
    "        'T1192': 'T1566.002',\n",
    "        'T1139': 'T1552.003',\n",
    "        'T1094': 'T1095',\n",
    "        'T1081': 'T1552.001',\n",
    "        'T1154': 'T1546.005',\n",
    "        'T1166': 'T1548.001',\n",
    "        'T1196': 'T1218.002',\n",
    "        'T1155': 'T1059.002',\n",
    "        'T1050': 'T1543.003',\n",
    "        'T1077': 'T1021.002',\n",
    "        'T1170': 'T1218.005',\n",
    "        'T1100': 'T1505.003',\n",
    "    }\n",
    "\n",
    "    # Step 3: Define deprecated techniques\n",
    "    deprecated_techniques = ['T1043']\n",
    "\n",
    "    # Step 5: Apply transformations\n",
    "    # Parse columns to ensure consistent format\n",
    "    original_df['True_labels'] = original_df['True_labels'].apply(safe_parse_labels)\n",
    "    original_df['Predicted_labels'] = original_df[predicted_header].apply(safe_parse_labels)\n",
    "\n",
    "    # Remove deprecated techniques from True_labels\n",
    "    original_df['True_labels'] = original_df['True_labels'].apply(\n",
    "        lambda x: [tech for tech in x if tech not in deprecated_techniques]\n",
    "    )\n",
    "    original_df = original_df[original_df['True_labels'].apply(len) > 0]  # Remove rows with empty True_labels\n",
    "\n",
    "    # Remap deprecated techniques to updated equivalents\n",
    "    original_df['True_labels'] = original_df['True_labels'].apply(lambda x: remap_techniques(x, manual_updates))\n",
    "    original_df['Predicted_labels'] = original_df['Predicted_labels'].apply(lambda x: remap_techniques(x, manual_updates))\n",
    "\n",
    "    # Convert to parent techniques and remove duplicates\n",
    "    original_df['True_labels'] = original_df['True_labels'].apply(convert_and_deduplicate)\n",
    "    original_df['Predicted_labels'] = original_df['Predicted_labels'].apply(convert_and_deduplicate)\n",
    "\n",
    "    # Map techniques to tactics\n",
    "    original_df['True_labels'] = original_df['True_labels'].apply(lambda x: map_techniques_to_tactics(x, technique_to_tactics_mapping))\n",
    "    original_df['Predicted_labels'] = original_df['Predicted_labels'].apply(lambda x: map_techniques_to_tactics(x, technique_to_tactics_mapping))\n",
    "\n",
    "    # Step 6: Save the final processed dataset\n",
    "    deduplicated_parent_techniques_output_path = os.path.join(os.path.dirname(file_path), 'tactics_' + str(os.path.basename(file_path)))\n",
    "    original_df.to_csv(deduplicated_parent_techniques_output_path, index=False)\n",
    "\n",
    "    print(f\"Final dataset with deduplicated parent techniques saved to: {deduplicated_parent_techniques_output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final dataset with deduplicated parent techniques saved to: GPT\\tactics_prompting_without_techniques_guide_zero_shot_with_limit.csv\n",
      "Final dataset with deduplicated parent techniques saved to: GPT\\tactics_prompting_without_techniques_guide_one_shot_with_limit.csv\n",
      "Final dataset with deduplicated parent techniques saved to: GPT\\tactics_prompting_without_techniques_guide_two_shot_with_limit.csv\n",
      "Final dataset with deduplicated parent techniques saved to: GPT\\tactics_prompting_with_techniques_guide_zero_shot_False.csv\n",
      "Final dataset with deduplicated parent techniques saved to: GPT\\tactics_prompting_with_techniques_guide_one_shot_False.csv\n",
      "Final dataset with deduplicated parent techniques saved to: GPT\\tactics_prompting_with_techniques_guide_two_shot_False.csv\n"
     ]
    }
   ],
   "source": [
    "map_to_tactics('GPT/prompting_without_techniques_guide_zero_shot_with_limit.csv', 'Technique_id')\n",
    "map_to_tactics('GPT/prompting_without_techniques_guide_one_shot_with_limit.csv', 'Technique_id')\n",
    "map_to_tactics('GPT/prompting_without_techniques_guide_two_shot_with_limit.csv', 'Technique_id')\n",
    "map_to_tactics('GPT/prompting_with_techniques_guide_zero_shot_False.csv', 'Without_Prompt_Limit_With_Competition_With_Limit_Return')\n",
    "map_to_tactics('GPT/prompting_with_techniques_guide_one_shot_False.csv', 'Without_Prompt_Limit_With_Competition_With_Limit_Return')\n",
    "map_to_tactics('GPT/prompting_with_techniques_guide_two_shot_False.csv', 'Without_Prompt_Limit_With_Competition_With_Limit_Return')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final dataset with deduplicated parent techniques saved to: Gemini\\tactics_prompting_without_techniques_guide_zero_shot_with_limit.csv\n",
      "Final dataset with deduplicated parent techniques saved to: Gemini\\tactics_prompting_without_techniques_guide_one_shot_with_limit.csv\n",
      "Final dataset with deduplicated parent techniques saved to: Gemini\\tactics_prompting_without_techniques_guide_two_shot_with_limit.csv\n",
      "Final dataset with deduplicated parent techniques saved to: Gemini\\tactics_prompting_with_techniques_guide_zero_shot_False.csv\n",
      "Final dataset with deduplicated parent techniques saved to: Gemini\\tactics_prompting_with_techniques_guide_one_shot_False.csv\n",
      "Final dataset with deduplicated parent techniques saved to: Gemini\\tactics_prompting_with_techniques_guide_two_shot_False.csv\n"
     ]
    }
   ],
   "source": [
    "map_to_tactics('Gemini/prompting_without_techniques_guide_zero_shot_with_limit.csv', 'Technique_id')\n",
    "map_to_tactics('Gemini/prompting_without_techniques_guide_one_shot_with_limit.csv', 'Technique_id')\n",
    "map_to_tactics('Gemini/prompting_without_techniques_guide_two_shot_with_limit.csv', 'Technique_id')\n",
    "map_to_tactics('Gemini/prompting_with_techniques_guide_zero_shot_False.csv', 'Without_Prompt_Limit_With_Competition_With_Limit_Return')\n",
    "map_to_tactics('Gemini/prompting_with_techniques_guide_one_shot_False.csv', 'Without_Prompt_Limit_With_Competition_With_Limit_Return')\n",
    "map_to_tactics('Gemini/prompting_with_techniques_guide_two_shot_False.csv', 'Without_Prompt_Limit_With_Competition_With_Limit_Return')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final dataset with deduplicated parent techniques saved to: Claude\\tactics_prompting_without_techniques_guide_zero_shot_with_limit.csv\n",
      "Final dataset with deduplicated parent techniques saved to: Claude\\tactics_prompting_without_techniques_guide_one_shot_with_limit.csv\n",
      "Final dataset with deduplicated parent techniques saved to: Claude\\tactics_prompting_without_techniques_guide_two_shot_with_limit.csv\n",
      "Final dataset with deduplicated parent techniques saved to: Claude\\tactics_prompting_with_techniques_guide_zero_shot_False.csv\n",
      "Final dataset with deduplicated parent techniques saved to: Claude\\tactics_prompting_with_techniques_guide_one_shot_False.csv\n",
      "Final dataset with deduplicated parent techniques saved to: Claude\\tactics_prompting_with_techniques_guide_two_shot_False.csv\n"
     ]
    }
   ],
   "source": [
    "map_to_tactics('Claude/prompting_without_techniques_guide_zero_shot_with_limit.csv', 'Technique_id')\n",
    "map_to_tactics('Claude/prompting_without_techniques_guide_one_shot_with_limit.csv', 'Technique_id')\n",
    "map_to_tactics('Claude/prompting_without_techniques_guide_two_shot_with_limit.csv', 'Technique_id')\n",
    "map_to_tactics('Claude/prompting_with_techniques_guide_zero_shot_False.csv', 'Without_Prompt_Limit_With_Competition_With_Limit_Return')\n",
    "map_to_tactics('Claude/prompting_with_techniques_guide_one_shot_False.csv', 'Without_Prompt_Limit_With_Competition_With_Limit_Return')\n",
    "map_to_tactics('Claude/prompting_with_techniques_guide_two_shot_False.csv', 'Without_Prompt_Limit_With_Competition_With_Limit_Return')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final dataset with deduplicated parent techniques saved to: ML\\tactics_ChatGPT_test_ground_truth_and_predictions.csv\n",
      "Final dataset with deduplicated parent techniques saved to: ML\\tactics_Claude_test_ground_truth_and_predictions.csv\n",
      "Final dataset with deduplicated parent techniques saved to: ML\\tactics_Gemini_test_ground_truth_and_predictions.csv\n"
     ]
    }
   ],
   "source": [
    "map_to_tactics('ML/ChatGPT_test_ground_truth_and_predictions.csv', 'Predicted_labels')\n",
    "map_to_tactics('ML/Claude_test_ground_truth_and_predictions.csv', 'Predicted_labels')\n",
    "map_to_tactics('ML/Gemini_test_ground_truth_and_predictions.csv', 'Predicted_labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "WlJZ7D0Pg8zQ"
   },
   "outputs": [],
   "source": [
    "import ast\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import math\n",
    "\n",
    "def evaluation(true_labels, predicted_labels):\n",
    "  from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "  import ast\n",
    "\n",
    "  # Convert string representations of lists into actual Python lists\n",
    "  predicted_labels = predicted_labels.apply(ast.literal_eval)\n",
    "  true_labels = true_labels.apply(ast.literal_eval)\n",
    "\n",
    "  # Create a flattened list of unique labels for multi-label metrics\n",
    "  unique_labels = set(label for sublist in true_labels for label in sublist).union(\n",
    "      set(label for sublist in true_labels for label in sublist)\n",
    "  )\n",
    "  unique_labels = sorted(unique_labels)\n",
    "\n",
    "  # Binarize the labels for multi-label metric calculation\n",
    "  def binarize_labels(label_list, classes):\n",
    "      return [1 if label in label_list else 0 for label in classes]\n",
    "\n",
    "  Binarized_Predicted = predicted_labels.apply(lambda x: binarize_labels(x, unique_labels))\n",
    "  Binarized_True = true_labels.apply(lambda x: binarize_labels(x, unique_labels))\n",
    "\n",
    "  # Stack the binary arrays for multi-label metric computation\n",
    "  y_pred = Binarized_Predicted.tolist()\n",
    "  y_true = Binarized_True.tolist()\n",
    "\n",
    "  # Calculate precision, recall, and F1-score\n",
    "  avg_precision = precision_score(y_true, y_pred, average='micro')\n",
    "  avg_recall = recall_score(y_true, y_pred, average='micro')\n",
    "  avg_f1 = f1_score(y_true, y_pred, average='micro')\n",
    "\n",
    "  avg_precision, avg_recall, avg_f1\n",
    "  print(\"Metric    |   Score\")\n",
    "  print(\"-------------------\")\n",
    "  print(f\"Precision |   {avg_precision:.2f}\")\n",
    "  print(f\"Recall    |   {avg_recall:.2f}\")\n",
    "  print(f\"F1 Score  |   {avg_f1:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WXhTA0p8al5g",
    "outputId": "b743f420-aaa9-4c16-9ef8-de0f2a4c7f0a"
   },
   "outputs": [],
   "source": [
    "def evaluate_tactics(file_path):\n",
    "    loadData = pd.read_csv(file_path)\n",
    "    true_labels_ZS = loadData['True_labels']\n",
    "    predicted_labels = loadData['Predicted_labels']\n",
    "    print(f\"Evaluating {file_path}\")\n",
    "    evaluation(true_labels_ZS, predicted_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N6rrCwGQhAMo",
    "outputId": "ea2ff142-d31e-4c47-8c25-3738fc7b738c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating GPT/tactics_prompting_without_techniques_guide_zero_shot_with_limit.csv\n",
      "Metric    |   Score\n",
      "-------------------\n",
      "Precision |   0.50\n",
      "Recall    |   0.45\n",
      "F1 Score  |   0.47\n",
      "Evaluating GPT/tactics_prompting_without_techniques_guide_one_shot_with_limit.csv\n",
      "Metric    |   Score\n",
      "-------------------\n",
      "Precision |   0.49\n",
      "Recall    |   0.47\n",
      "F1 Score  |   0.48\n",
      "Evaluating GPT/tactics_prompting_without_techniques_guide_two_shot_with_limit.csv\n",
      "Metric    |   Score\n",
      "-------------------\n",
      "Precision |   0.44\n",
      "Recall    |   0.50\n",
      "F1 Score  |   0.47\n",
      "Evaluating GPT/tactics_prompting_with_techniques_guide_zero_shot_False.csv\n",
      "Metric    |   Score\n",
      "-------------------\n",
      "Precision |   0.62\n",
      "Recall    |   0.69\n",
      "F1 Score  |   0.65\n",
      "Evaluating GPT/tactics_prompting_with_techniques_guide_one_shot_False.csv\n",
      "Metric    |   Score\n",
      "-------------------\n",
      "Precision |   0.62\n",
      "Recall    |   0.69\n",
      "F1 Score  |   0.65\n",
      "Evaluating GPT/tactics_prompting_with_techniques_guide_two_shot_False.csv\n",
      "Metric    |   Score\n",
      "-------------------\n",
      "Precision |   0.66\n",
      "Recall    |   0.72\n",
      "F1 Score  |   0.69\n"
     ]
    }
   ],
   "source": [
    "evaluate_tactics('GPT/tactics_prompting_without_techniques_guide_zero_shot_with_limit.csv')\n",
    "evaluate_tactics('GPT/tactics_prompting_without_techniques_guide_one_shot_with_limit.csv')\n",
    "evaluate_tactics('GPT/tactics_prompting_without_techniques_guide_two_shot_with_limit.csv')\n",
    "evaluate_tactics('GPT/tactics_prompting_with_techniques_guide_zero_shot_False.csv')\n",
    "evaluate_tactics('GPT/tactics_prompting_with_techniques_guide_one_shot_False.csv')\n",
    "evaluate_tactics('GPT/tactics_prompting_with_techniques_guide_two_shot_False.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Gemini/tactics_prompting_without_techniques_guide_zero_shot_with_limit.csv\n",
      "Metric    |   Score\n",
      "-------------------\n",
      "Precision |   0.24\n",
      "Recall    |   0.26\n",
      "F1 Score  |   0.25\n",
      "Evaluating Gemini/tactics_prompting_without_techniques_guide_one_shot_with_limit.csv\n",
      "Metric    |   Score\n",
      "-------------------\n",
      "Precision |   0.34\n",
      "Recall    |   0.39\n",
      "F1 Score  |   0.36\n",
      "Evaluating Gemini/tactics_prompting_without_techniques_guide_two_shot_with_limit.csv\n",
      "Metric    |   Score\n",
      "-------------------\n",
      "Precision |   0.29\n",
      "Recall    |   0.28\n",
      "F1 Score  |   0.29\n",
      "Evaluating Gemini/tactics_prompting_with_techniques_guide_zero_shot_False.csv\n",
      "Metric    |   Score\n",
      "-------------------\n",
      "Precision |   0.38\n",
      "Recall    |   0.45\n",
      "F1 Score  |   0.41\n",
      "Evaluating Gemini/tactics_prompting_with_techniques_guide_one_shot_False.csv\n",
      "Metric    |   Score\n",
      "-------------------\n",
      "Precision |   0.50\n",
      "Recall    |   0.66\n",
      "F1 Score  |   0.57\n",
      "Evaluating Gemini/tactics_prompting_with_techniques_guide_two_shot_False.csv\n",
      "Metric    |   Score\n",
      "-------------------\n",
      "Precision |   0.46\n",
      "Recall    |   0.60\n",
      "F1 Score  |   0.52\n"
     ]
    }
   ],
   "source": [
    "evaluate_tactics('Gemini/tactics_prompting_without_techniques_guide_zero_shot_with_limit.csv')\n",
    "evaluate_tactics('Gemini/tactics_prompting_without_techniques_guide_one_shot_with_limit.csv')\n",
    "evaluate_tactics('Gemini/tactics_prompting_without_techniques_guide_two_shot_with_limit.csv')\n",
    "evaluate_tactics('Gemini/tactics_prompting_with_techniques_guide_zero_shot_False.csv')\n",
    "evaluate_tactics('Gemini/tactics_prompting_with_techniques_guide_one_shot_False.csv')\n",
    "evaluate_tactics('Gemini/tactics_prompting_with_techniques_guide_two_shot_False.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Claude/tactics_prompting_without_techniques_guide_zero_shot_with_limit.csv\n",
      "Metric    |   Score\n",
      "-------------------\n",
      "Precision |   0.39\n",
      "Recall    |   0.57\n",
      "F1 Score  |   0.46\n",
      "Evaluating Claude/tactics_prompting_without_techniques_guide_one_shot_with_limit.csv\n",
      "Metric    |   Score\n",
      "-------------------\n",
      "Precision |   0.39\n",
      "Recall    |   0.60\n",
      "F1 Score  |   0.47\n",
      "Evaluating Claude/tactics_prompting_without_techniques_guide_two_shot_with_limit.csv\n",
      "Metric    |   Score\n",
      "-------------------\n",
      "Precision |   0.42\n",
      "Recall    |   0.64\n",
      "F1 Score  |   0.51\n",
      "Evaluating Claude/tactics_prompting_with_techniques_guide_zero_shot_False.csv\n",
      "Metric    |   Score\n",
      "-------------------\n",
      "Precision |   0.57\n",
      "Recall    |   0.79\n",
      "F1 Score  |   0.66\n",
      "Evaluating Claude/tactics_prompting_with_techniques_guide_one_shot_False.csv\n",
      "Metric    |   Score\n",
      "-------------------\n",
      "Precision |   0.59\n",
      "Recall    |   0.80\n",
      "F1 Score  |   0.68\n",
      "Evaluating Claude/tactics_prompting_with_techniques_guide_two_shot_False.csv\n",
      "Metric    |   Score\n",
      "-------------------\n",
      "Precision |   0.61\n",
      "Recall    |   0.80\n",
      "F1 Score  |   0.69\n"
     ]
    }
   ],
   "source": [
    "evaluate_tactics('Claude/tactics_prompting_without_techniques_guide_zero_shot_with_limit.csv')\n",
    "evaluate_tactics('Claude/tactics_prompting_without_techniques_guide_one_shot_with_limit.csv')\n",
    "evaluate_tactics('Claude/tactics_prompting_without_techniques_guide_two_shot_with_limit.csv')\n",
    "evaluate_tactics('Claude/tactics_prompting_with_techniques_guide_zero_shot_False.csv')\n",
    "evaluate_tactics('Claude/tactics_prompting_with_techniques_guide_one_shot_False.csv')\n",
    "evaluate_tactics('Claude/tactics_prompting_with_techniques_guide_two_shot_False.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating ML/tactics_ChatGPT_test_ground_truth_and_predictions.csv\n",
      "Metric    |   Score\n",
      "-------------------\n",
      "Precision |   0.96\n",
      "Recall    |   0.77\n",
      "F1 Score  |   0.85\n",
      "Evaluating ML/tactics_Claude_test_ground_truth_and_predictions.csv\n",
      "Metric    |   Score\n",
      "-------------------\n",
      "Precision |   0.91\n",
      "Recall    |   0.92\n",
      "F1 Score  |   0.91\n",
      "Evaluating ML/tactics_Gemini_test_ground_truth_and_predictions.csv\n",
      "Metric    |   Score\n",
      "-------------------\n",
      "Precision |   0.91\n",
      "Recall    |   0.92\n",
      "F1 Score  |   0.92\n"
     ]
    }
   ],
   "source": [
    "evaluate_tactics('ML/tactics_ChatGPT_test_ground_truth_and_predictions.csv')\n",
    "evaluate_tactics('ML/tactics_Claude_test_ground_truth_and_predictions.csv')\n",
    "evaluate_tactics('ML/tactics_Gemini_test_ground_truth_and_predictions.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "# Step 4: Define functions for transformations\n",
    "def safe_parse_labels(labels):\n",
    "    \"\"\"Safely parse labels from a string or return them as-is if already a list.\"\"\"\n",
    "    if isinstance(labels, str):  # If it's a string, evaluate it\n",
    "        try:\n",
    "            return ast.literal_eval(labels)\n",
    "        except (ValueError, SyntaxError):\n",
    "            return []\n",
    "    elif isinstance(labels, list):  # If it's already a list, return as is\n",
    "        return labels\n",
    "    return []\n",
    "\n",
    "def remap_techniques(labels, updates):\n",
    "    \"\"\"Remap techniques to updated equivalents using manual_updates.\"\"\"\n",
    "    return [updates.get(tech, tech) for tech in labels]\n",
    "\n",
    "def convert_and_deduplicate(labels):\n",
    "    \"\"\"Convert sub-techniques to parent techniques and remove duplicates.\"\"\"\n",
    "    return list(set([tech.split('.')[0] if '.' in tech else tech for tech in labels]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "import os\n",
    "\n",
    "def map_to_techniques(file_path, predicted_header):\n",
    "    # Load the original dataset\n",
    "    original_df = pd.read_csv(file_path)\n",
    "\n",
    "    # Load the updated MITRE ATT&CK Enterprise data\n",
    "    updated_mitre_data_path = 'enterprise-attack-v16.1 (1).xlsx'\n",
    "    updated_mitre_data = pd.read_excel(updated_mitre_data_path, sheet_name=None)\n",
    "\n",
    "    # Extract necessary sheets\n",
    "    updated_techniques_df = updated_mitre_data['techniques']\n",
    "    tactics_df = updated_mitre_data['tactics']\n",
    "\n",
    "    # Step 1: Create a mapping from techniques to tactics\n",
    "    technique_to_tactics_mapping = {}\n",
    "    for index, row in updated_techniques_df.iterrows():\n",
    "        technique_id = row['ID']\n",
    "        if pd.notnull(row['tactics']):\n",
    "            tactics = [tactic.strip() for tactic in row['tactics'].split(',')]\n",
    "            tactic_ids = [tactics_df[tactics_df['name'] == tactic]['ID'].values[0] for tactic in tactics]\n",
    "            technique_to_tactics_mapping[technique_id] = tactic_ids\n",
    "\n",
    "    # Step 2: Define user-provided manual remappings for deprecated techniques\n",
    "    manual_updates = {\n",
    "        'T1192': 'T1566.002',\n",
    "        'T1139': 'T1552.003',\n",
    "        'T1094': 'T1095',\n",
    "        'T1081': 'T1552.001',\n",
    "        'T1154': 'T1546.005',\n",
    "        'T1166': 'T1548.001',\n",
    "        'T1196': 'T1218.002',\n",
    "        'T1155': 'T1059.002',\n",
    "        'T1050': 'T1543.003',\n",
    "        'T1077': 'T1021.002',\n",
    "        'T1170': 'T1218.005',\n",
    "        'T1100': 'T1505.003',\n",
    "    }\n",
    "\n",
    "    # Step 3: Define deprecated techniques\n",
    "    deprecated_techniques = ['T1043']\n",
    "\n",
    "    # Step 5: Apply transformations\n",
    "    # Parse columns to ensure consistent format\n",
    "    original_df['True_labels'] = original_df['True_labels'].apply(safe_parse_labels)\n",
    "    original_df['Predicted_labels'] = original_df[predicted_header].apply(safe_parse_labels)\n",
    "\n",
    "    # Remove deprecated techniques from True_labels\n",
    "    original_df['True_labels'] = original_df['True_labels'].apply(\n",
    "        lambda x: [tech for tech in x if tech not in deprecated_techniques]\n",
    "    )\n",
    "    original_df = original_df[original_df['True_labels'].apply(len) > 0]  # Remove rows with empty True_labels\n",
    "\n",
    "    # Remap deprecated techniques to updated equivalents\n",
    "    original_df['True_labels'] = original_df['True_labels'].apply(lambda x: remap_techniques(x, manual_updates))\n",
    "    original_df['Predicted_labels'] = original_df['Predicted_labels'].apply(lambda x: remap_techniques(x, manual_updates))\n",
    "\n",
    "    # Convert to parent techniques and remove duplicates\n",
    "    original_df['True_labels'] = original_df['True_labels'].apply(convert_and_deduplicate)\n",
    "    original_df['Predicted_labels'] = original_df['Predicted_labels'].apply(convert_and_deduplicate)\n",
    "\n",
    "    # Step 6: Save the final processed dataset\n",
    "    deduplicated_parent_techniques_output_path = os.path.join(os.path.dirname(file_path), 'tactics_' + str(os.path.basename(file_path)))\n",
    "    original_df.to_csv(deduplicated_parent_techniques_output_path, index=False)\n",
    "\n",
    "    print(f\"Final dataset with deduplicated parent techniques saved to: {deduplicated_parent_techniques_output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_techniques(file_path, predicted_header):\n",
    "    loadData = pd.read_csv(file_path)\n",
    "    true_labels_ZS = loadData['True_labels']\n",
    "    predicted_labels = loadData[predicted_header]\n",
    "    print(f\"Evaluating {file_path}\")\n",
    "    evaluation(true_labels_ZS, predicted_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Gemini/prompting_without_techniques_guide_zero_shot_with_limit.csv\n",
      "Metric    |   Score\n",
      "-------------------\n",
      "Precision |   0.20\n",
      "Recall    |   0.14\n",
      "F1 Score  |   0.16\n",
      "Evaluating Gemini/prompting_without_techniques_guide_one_shot_with_limit.csv\n",
      "Metric    |   Score\n",
      "-------------------\n",
      "Precision |   0.34\n",
      "Recall    |   0.24\n",
      "F1 Score  |   0.28\n",
      "Evaluating Gemini/prompting_without_techniques_guide_two_shot_with_limit.csv\n",
      "Metric    |   Score\n",
      "-------------------\n",
      "Precision |   0.30\n",
      "Recall    |   0.18\n",
      "F1 Score  |   0.23\n",
      "Evaluating Gemini/prompting_with_techniques_guide_zero_shot_False.csv\n",
      "Metric    |   Score\n",
      "-------------------\n",
      "Precision |   0.41\n",
      "Recall    |   0.28\n",
      "F1 Score  |   0.33\n",
      "Evaluating Gemini/prompting_with_techniques_guide_one_shot_False.csv\n",
      "Metric    |   Score\n",
      "-------------------\n",
      "Precision |   0.60\n",
      "Recall    |   0.48\n",
      "F1 Score  |   0.53\n",
      "Evaluating Gemini/prompting_with_techniques_guide_two_shot_False.csv\n",
      "Metric    |   Score\n",
      "-------------------\n",
      "Precision |   0.55\n",
      "Recall    |   0.43\n",
      "F1 Score  |   0.48\n"
     ]
    }
   ],
   "source": [
    "evaluate_techniques('Gemini/prompting_without_techniques_guide_zero_shot_with_limit.csv', 'Technique_id')\n",
    "evaluate_techniques('Gemini/prompting_without_techniques_guide_one_shot_with_limit.csv', 'Technique_id')\n",
    "evaluate_techniques('Gemini/prompting_without_techniques_guide_two_shot_with_limit.csv', 'Technique_id')\n",
    "evaluate_techniques('Gemini/prompting_with_techniques_guide_zero_shot_False.csv', 'Without_Prompt_Limit_With_Competition_With_Limit_Return')\n",
    "evaluate_techniques('Gemini/prompting_with_techniques_guide_one_shot_False.csv', 'Without_Prompt_Limit_With_Competition_With_Limit_Return')\n",
    "evaluate_techniques('Gemini/prompting_with_techniques_guide_two_shot_False.csv', 'Without_Prompt_Limit_With_Competition_With_Limit_Return')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Claude/prompting_without_techniques_guide_zero_shot_with_limit.csv\n",
      "Metric    |   Score\n",
      "-------------------\n",
      "Precision |   0.29\n",
      "Recall    |   0.40\n",
      "F1 Score  |   0.34\n",
      "Evaluating Claude/prompting_without_techniques_guide_one_shot_with_limit.csv\n",
      "Metric    |   Score\n",
      "-------------------\n",
      "Precision |   0.28\n",
      "Recall    |   0.41\n",
      "F1 Score  |   0.33\n",
      "Evaluating Claude/prompting_without_techniques_guide_two_shot_with_limit.csv\n",
      "Metric    |   Score\n",
      "-------------------\n",
      "Precision |   0.31\n",
      "Recall    |   0.42\n",
      "F1 Score  |   0.36\n",
      "Evaluating Claude/prompting_with_techniques_guide_zero_shot_False.csv\n",
      "Metric    |   Score\n",
      "-------------------\n",
      "Precision |   0.51\n",
      "Recall    |   0.59\n",
      "F1 Score  |   0.55\n",
      "Evaluating Claude/prompting_with_techniques_guide_one_shot_False.csv\n",
      "Metric    |   Score\n",
      "-------------------\n",
      "Precision |   0.57\n",
      "Recall    |   0.60\n",
      "F1 Score  |   0.59\n",
      "Evaluating Claude/prompting_with_techniques_guide_two_shot_False.csv\n",
      "Metric    |   Score\n",
      "-------------------\n",
      "Precision |   0.63\n",
      "Recall    |   0.61\n",
      "F1 Score  |   0.62\n"
     ]
    }
   ],
   "source": [
    "evaluate_techniques('Claude/prompting_without_techniques_guide_zero_shot_with_limit.csv', 'Technique_id')\n",
    "evaluate_techniques('Claude/prompting_without_techniques_guide_one_shot_with_limit.csv', 'Technique_id')\n",
    "evaluate_techniques('Claude/prompting_without_techniques_guide_two_shot_with_limit.csv', 'Technique_id')\n",
    "evaluate_techniques('Claude/prompting_with_techniques_guide_zero_shot_False.csv', 'Without_Prompt_Limit_With_Competition_With_Limit_Return')\n",
    "evaluate_techniques('Claude/prompting_with_techniques_guide_one_shot_False.csv', 'Without_Prompt_Limit_With_Competition_With_Limit_Return')\n",
    "evaluate_techniques('Claude/prompting_with_techniques_guide_two_shot_False.csv', 'Without_Prompt_Limit_With_Competition_With_Limit_Return')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating GPT/prompting_without_techniques_guide_zero_shot_with_limit.csv\n",
      "Metric    |   Score\n",
      "-------------------\n",
      "Precision |   0.45\n",
      "Recall    |   0.27\n",
      "F1 Score  |   0.34\n",
      "Evaluating GPT/prompting_without_techniques_guide_one_shot_with_limit.csv\n",
      "Metric    |   Score\n",
      "-------------------\n",
      "Precision |   0.46\n",
      "Recall    |   0.29\n",
      "F1 Score  |   0.36\n",
      "Evaluating GPT/prompting_without_techniques_guide_two_shot_with_limit.csv\n",
      "Metric    |   Score\n",
      "-------------------\n",
      "Precision |   0.35\n",
      "Recall    |   0.30\n",
      "F1 Score  |   0.32\n",
      "Evaluating GPT/prompting_with_techniques_guide_zero_shot_False.csv\n",
      "Metric    |   Score\n",
      "-------------------\n",
      "Precision |   0.56\n",
      "Recall    |   0.55\n",
      "F1 Score  |   0.56\n",
      "Evaluating GPT/prompting_with_techniques_guide_one_shot_False.csv\n",
      "Metric    |   Score\n",
      "-------------------\n",
      "Precision |   0.66\n",
      "Recall    |   0.58\n",
      "F1 Score  |   0.62\n",
      "Evaluating GPT/prompting_with_techniques_guide_two_shot_False.csv\n",
      "Metric    |   Score\n",
      "-------------------\n",
      "Precision |   0.70\n",
      "Recall    |   0.55\n",
      "F1 Score  |   0.62\n"
     ]
    }
   ],
   "source": [
    "evaluate_techniques('GPT/prompting_without_techniques_guide_zero_shot_with_limit.csv', 'Technique_id')\n",
    "evaluate_techniques('GPT/prompting_without_techniques_guide_one_shot_with_limit.csv', 'Technique_id')\n",
    "evaluate_techniques('GPT/prompting_without_techniques_guide_two_shot_with_limit.csv', 'Technique_id')\n",
    "evaluate_techniques('GPT/prompting_with_techniques_guide_zero_shot_False.csv', 'Without_Prompt_Limit_With_Competition_With_Limit_Return')\n",
    "evaluate_techniques('GPT/prompting_with_techniques_guide_one_shot_False.csv', 'Without_Prompt_Limit_With_Competition_With_Limit_Return')\n",
    "evaluate_techniques('GPT/prompting_with_techniques_guide_two_shot_False.csv', 'Without_Prompt_Limit_With_Competition_With_Limit_Return')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating ML/ChatGPT_test_ground_truth_and_predictions.csv\n",
      "Metric    |   Score\n",
      "-------------------\n",
      "Precision |   0.92\n",
      "Recall    |   0.70\n",
      "F1 Score  |   0.79\n",
      "Evaluating ML/Claude_test_ground_truth_and_predictions.csv\n",
      "Metric    |   Score\n",
      "-------------------\n",
      "Precision |   0.86\n",
      "Recall    |   0.85\n",
      "F1 Score  |   0.85\n",
      "Evaluating ML/Gemini_test_ground_truth_and_predictions.csv\n",
      "Metric    |   Score\n",
      "-------------------\n",
      "Precision |   0.88\n",
      "Recall    |   0.87\n",
      "F1 Score  |   0.87\n"
     ]
    }
   ],
   "source": [
    "evaluate_techniques('ML/ChatGPT_test_ground_truth_and_predictions.csv', 'Predicted_labels')\n",
    "evaluate_techniques('ML/Claude_test_ground_truth_and_predictions.csv', 'Predicted_labels')\n",
    "evaluate_techniques('ML/Gemini_test_ground_truth_and_predictions.csv', 'Predicted_labels')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rare Techniques Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Gemini/rare_prompting_with_techniques_guide_two_shot_False.csv\n",
      "Metric    |   Score\n",
      "-------------------\n",
      "Precision |   0.22\n",
      "Recall    |   0.21\n",
      "F1 Score  |   0.22\n",
      "Evaluating Claude/rare_prompting_with_techniques_guide_two_shot_False.csv\n",
      "Metric    |   Score\n",
      "-------------------\n",
      "Precision |   0.23\n",
      "Recall    |   0.18\n",
      "F1 Score  |   0.20\n",
      "Evaluating GPT/rare_prompting_with_techniques_guide_two_shot_False.csv\n",
      "Metric    |   Score\n",
      "-------------------\n",
      "Precision |   0.29\n",
      "Recall    |   0.19\n",
      "F1 Score  |   0.23\n"
     ]
    }
   ],
   "source": [
    "evaluate_techniques('Gemini/rare_prompting_with_techniques_guide_two_shot_False.csv', 'Without_Prompt_Limit_With_Competition_With_Limit_Return')\n",
    "evaluate_techniques('Claude/rare_prompting_with_techniques_guide_two_shot_False.csv', 'Without_Prompt_Limit_With_Competition_With_Limit_Return')\n",
    "evaluate_techniques('GPT/rare_prompting_with_techniques_guide_two_shot_False.csv', 'Without_Prompt_Limit_With_Competition_With_Limit_Return')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final dataset with deduplicated parent techniques saved to: Gemini\\tactics_rare_prompting_with_techniques_guide_two_shot_False.csv\n",
      "Final dataset with deduplicated parent techniques saved to: Claude\\tactics_rare_prompting_with_techniques_guide_two_shot_False.csv\n",
      "Final dataset with deduplicated parent techniques saved to: GPT\\tactics_rare_prompting_with_techniques_guide_two_shot_False.csv\n"
     ]
    }
   ],
   "source": [
    "map_to_tactics('Gemini/rare_prompting_with_techniques_guide_two_shot_False.csv', 'Without_Prompt_Limit_With_Competition_With_Limit_Return')\n",
    "map_to_tactics('Claude/rare_prompting_with_techniques_guide_two_shot_False.csv', 'Without_Prompt_Limit_With_Competition_With_Limit_Return')\n",
    "map_to_tactics('GPT/rare_prompting_with_techniques_guide_two_shot_False.csv', 'Without_Prompt_Limit_With_Competition_With_Limit_Return')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Gemini/tactics_rare_prompting_with_techniques_guide_two_shot_False.csv\n",
      "Metric    |   Score\n",
      "-------------------\n",
      "Precision |   0.29\n",
      "Recall    |   0.38\n",
      "F1 Score  |   0.33\n",
      "Evaluating Claude/tactics_rare_prompting_with_techniques_guide_two_shot_False.csv\n",
      "Metric    |   Score\n",
      "-------------------\n",
      "Precision |   0.26\n",
      "Recall    |   0.29\n",
      "F1 Score  |   0.27\n",
      "Evaluating GPT/tactics_rare_prompting_with_techniques_guide_two_shot_False.csv\n",
      "Metric    |   Score\n",
      "-------------------\n",
      "Precision |   0.28\n",
      "Recall    |   0.26\n",
      "F1 Score  |   0.27\n"
     ]
    }
   ],
   "source": [
    "evaluate_tactics('Gemini/tactics_rare_prompting_with_techniques_guide_two_shot_False.csv')\n",
    "evaluate_tactics('Claude/tactics_rare_prompting_with_techniques_guide_two_shot_False.csv')\n",
    "evaluate_tactics('GPT/tactics_rare_prompting_with_techniques_guide_two_shot_False.csv')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "I7DjcZeOZ4nR",
    "-D9rfL2roNR9",
    "nz_5UVOypq2-",
    "s-8i8ebSpqxG",
    "Hm1LPasMKH_V",
    "T0dV1xXnpqrv",
    "9PDyuWeDpqkn",
    "7aOvvGzApqUQ",
    "QqzXpySOtBFs",
    "Au3iaHvfg2_g",
    "zVkzHkKEysVX",
    "i7CFd816akx4",
    "AvJSwkHdzDvv",
    "N4aLDGzriV55"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
